{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def community_detection(embeddings, threshold=0.75, min_community_size=10, init_max_size=1000):\n",
    "    \"\"\"\n",
    "    Function for Fast Community Detection\n",
    "    Finds in the embeddings all communities, i.e. embeddings that are close (closer than threshold).\n",
    "    Returns only communities that are larger than min_community_size. The communities are returned\n",
    "    in decreasing order. The first element in each list is the central point in the community.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute cosine similarity scores\n",
    "    cos_scores = util.pytorch_cos_sim(embeddings, embeddings)\n",
    "\n",
    "    # Minimum size for a community\n",
    "    top_k_values, _ = cos_scores.topk(k=min_community_size, largest=True)\n",
    "\n",
    "    # Filter for rows >= min_threshold\n",
    "    extracted_communities = []\n",
    "    for i in range(len(top_k_values)):\n",
    "        if top_k_values[i][-1] >= threshold:\n",
    "            new_cluster = []\n",
    "\n",
    "            # Only check top k most similar entries\n",
    "            top_val_large, top_idx_large = cos_scores[i].topk(k=init_max_size, largest=True)\n",
    "            top_idx_large = top_idx_large.tolist()\n",
    "            top_val_large = top_val_large.tolist()\n",
    "\n",
    "            if top_val_large[-1] < threshold:\n",
    "                for idx, val in zip(top_idx_large, top_val_large):\n",
    "                    if val < threshold:\n",
    "                        break\n",
    "\n",
    "                    new_cluster.append(idx)\n",
    "            else:\n",
    "                # Iterate over all entries (slow)\n",
    "                for idx, val in enumerate(cos_scores[i].tolist()):\n",
    "                    if val >= threshold:\n",
    "                        new_cluster.append(idx)\n",
    "\n",
    "            extracted_communities.append(new_cluster)\n",
    "\n",
    "    # Largest cluster first\n",
    "    extracted_communities = sorted(extracted_communities, key=lambda x: len(x), reverse=True)\n",
    "\n",
    "    # Step 2) Remove overlapping communities\n",
    "    unique_communities = []\n",
    "    extracted_ids = set()\n",
    "\n",
    "    for community in extracted_communities:\n",
    "        add_cluster = True\n",
    "        for idx in community:\n",
    "            if idx in extracted_ids:\n",
    "                add_cluster = False\n",
    "                break\n",
    "\n",
    "        if add_cluster:\n",
    "            unique_communities.append(community)\n",
    "            for idx in community:\n",
    "                extracted_ids.add(idx)\n",
    "\n",
    "    return unique_communities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model for computing sentence embeddings. We use one trained for similar questions detection\n",
    "model = SentenceTransformer('distilbert-base-nli-stsb-quora-ranking')\n",
    "\n",
    "# We donwload the Quora Duplicate Questions Dataset (https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs)\n",
    "# and find similar question in it\n",
    "url = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\n",
    "dataset_path = \"quora_duplicate_questions.tsv\"\n",
    "max_corpus_size = 50000 # We limit our corpus to only the first 50k questions\n",
    "embedding_cache_path = 'quora-embeddings-size-{}.pkl'.format(max_corpus_size)\n",
    "\n",
    "#Check if embedding cache path exists\n",
    "if not os.path.exists(embedding_cache_path):\n",
    "    # Check if the dataset exists. If not, download and extract\n",
    "    # Download dataset if needed\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(\"Download dataset\")\n",
    "        util.http_get(url, dataset_path)\n",
    "\n",
    "    # Get all unique sentences from the file\n",
    "    corpus_sentences = set()\n",
    "    with open(dataset_path, encoding='utf8') as fIn:\n",
    "        reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in reader:\n",
    "            corpus_sentences.add(row['question1'])\n",
    "            if len(corpus_sentences) >= max_corpus_size:\n",
    "                break\n",
    "\n",
    "            corpus_sentences.add(row['question2'])\n",
    "            if len(corpus_sentences) >= max_corpus_size:\n",
    "                break\n",
    "\n",
    "    corpus_sentences = list(corpus_sentences)\n",
    "    print(\"Encode the corpus. This might take a while\")\n",
    "    corpus_embeddings = model.encode(corpus_sentences, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "    print(\"Store file on disc\")\n",
    "    with open(embedding_cache_path, \"wb\") as fOut:\n",
    "        pickle.dump({'sentences': corpus_sentences, 'embeddings': corpus_embeddings}, fOut)\n",
    "else:\n",
    "    print(\"Load pre-computed embeddings from disc\")\n",
    "    with open(embedding_cache_path, \"rb\") as fIn:\n",
    "        cache_data = pickle.load(fIn)\n",
    "        corpus_sentences = cache_data['sentences']\n",
    "        corpus_embeddings = cache_data['embeddings']\n",
    "\n",
    "print(\"Start clustering\")\n",
    "start_time = time.time()\n",
    "\n",
    "#Two parameter to tune:\n",
    "#min_cluster_size: Only consider cluster that have at least 25 elements (30 similar sentences)\n",
    "#threshold: Consider sentence pairs with a cosine-similarity larger than threshold as similar\n",
    "clusters = community_detection(corpus_embeddings, min_community_size=25, threshold=0.95)\n",
    "\n",
    "\n",
    "#Print all cluster / communities\n",
    "for i, cluster in enumerate(clusters):\n",
    "    print(\"\\nCluster {}, #{} Elements \".format(i+1, len(cluster)))\n",
    "    for sentence_id in cluster:\n",
    "        print(\"\\t\", corpus_sentences[sentence_id])\n",
    "\n",
    "\n",
    "\n",
    "print(\"Clustering done after {:.2f} sec\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
